{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study the cost of ensemble versus the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# modify this to set up directory:\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "model = \"resnet56\"\n",
    "attack_list = [\"losstraj\", \"reference\", \"lira\", \"calibration\"]\n",
    "# dataset_list = [\"cifar10\", \"cifar100\", \"cinic10\", \"texas100\", \"purchase100\"]\n",
    "dataset_list = [\"cifar10\", \"cifar100\", \"cinic10\"]\n",
    "ensemble_method_list = [\"union\", \"intersection\", \"majority_vote\"]\n",
    "seeds = [0, 1, 2, 3, 4, 5]\n",
    "path_to_data = f'{DATA_DIR}/miae_standard_exp'\n",
    "path_to_save_result = f'{path_to_data}/ensemble_roc/{model}/cost_perf_analysis'\n",
    "if os.path.exists(path_to_save_result) == False:\n",
    "    os.makedirs(path_to_save_result)\n",
    "\n",
    "ensemble_markers_mapping = {\n",
    "    ('calibration', 'lira'): 'o',          # Circle\n",
    "    ('calibration', 'losstraj'): 's',      # Square\n",
    "    ('calibration', 'reference'): 'D',     # Diamond\n",
    "    ('lira', 'losstraj'): '^',             # Triangle Up\n",
    "    ('lira', 'reference'): 'v',            # Triangle Down\n",
    "    ('losstraj', 'reference'): '<',        # Triangle Left\n",
    "    ('calibration', 'lira', 'losstraj'): '>',    # Triangle Right\n",
    "    ('calibration', 'lira', 'reference'): 'P',   # Plus (filled)\n",
    "    ('calibration', 'losstraj', 'reference'): '*', # Star\n",
    "    ('lira', 'losstraj', 'reference'): 'h',      # Hexagon1\n",
    "    ('calibration', 'lira', 'losstraj', 'reference'): 'H'  # Hexagon2\n",
    "}\n",
    "\n",
    "def get_marker(row):\n",
    "    attacks = []\n",
    "    if row[\"losstraj\"]:\n",
    "        attacks.append(\"losstraj\")\n",
    "    if row[\"reference\"]:\n",
    "        attacks.append(\"reference\")\n",
    "    if row[\"lira\"]:\n",
    "        attacks.append(\"lira\")\n",
    "    if row[\"calibration\"]:\n",
    "        attacks.append(\"calibration\")\n",
    "    attacks = sorted(attacks)\n",
    "    return ensemble_markers_mapping[tuple(attacks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the runtime (cost) of each attack. Attacks are ran on with 2 L40s GPUs in parallel. The GPU's utilization is kept below 100% to avoid any performance degradation. The settings for shadow model and target model are all resnet56\n",
    "\n",
    "breakdown of time for each attack:\n",
    "- losstraj: 17 \n",
    "- reference: 540 (for 20 shadow models) + 8 (for inference on each shadow model) = 548\n",
    "- lira: 540 (for 20 shadow models) + 40 (for augmented queries inference on each shadow model) = 580\n",
    "- calibration: 5\n",
    "\n",
    "The cost of LIRA and reference are so high because they trains 20 shadow models on-line. Meaning that both samples of inference and sample for training are used. Whereas losstraj and calibration only use auxiliary data for preparing attack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cost of each attack in terms of time (minutes)\n",
    "cost_table_time = {\"losstraj\": 17, \n",
    "              \"reference\": 548, \n",
    "              \"lira\": 580, \n",
    "              \"calibration\": 5,\n",
    "              \"lira_shadow_models\": 540\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ensemble_perf(dataset, num_seed, ensemble_method, path_to_data):\n",
    "    path_to_df = f\"{path_to_data}/ensemble_roc/{model}/{dataset}/{num_seed}_seeds/{ensemble_method}\"\n",
    "    df = pd.read_pickle(f\"{path_to_df}/ensemble_perf.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert roc and acc of multiple dataset to a csv table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "perf_union_df = pd.DataFrame(columns=[\"losstraj\", \"reference\", \"lira\", \"calibration\", \"dataset\", \"AUC\", \"ACC\", \"cost\", \"num_instance\", \"TPR@0.001FPR\"]\n",
    "                               ).astype({\"AUC\": float, \"ACC\": float, \"losstraj\": bool, \"reference\": bool, \"lira\": bool, \n",
    "                                         \"calibration\": bool, \"cost\": int, \"num_instance\": int, \"TPR@0.001FPR\": float})\n",
    "perf_intersection_df = deepcopy(perf_union_df)\n",
    "perf_mv_df = deepcopy(perf_union_df)\n",
    "\n",
    "for num_seed in range(2, len(seeds)+1):\n",
    "    # if num_seed % 2 == 0: # used for majority vote, since it requires odd number of seeds\n",
    "    #     continue\n",
    "    \n",
    "    # merge to a single dataframe\n",
    "    # rows: Ensemble Level, losstraj, reference, lira, calibration, dataset, ensemble_method, auc, acc\n",
    "    for ensemble in ensemble_method_list:\n",
    "        if ensemble == \"majority_vote\":\n",
    "            perf_df = perf_mv_df\n",
    "        elif ensemble == \"union\":\n",
    "            perf_df = perf_union_df\n",
    "        elif ensemble == \"intersection\":\n",
    "            perf_df = perf_intersection_df\n",
    "\n",
    "        for dataset in dataset_list:\n",
    "            df = load_ensemble_perf(dataset, num_seed, ensemble, path_to_data)\n",
    "            for _, row in df.iterrows():\n",
    "                auc = row[\"AUC\"]\n",
    "                acc = row[\"ACC\"]\n",
    "                attack_names = row[\"Attack\"].split(\"_\")\n",
    "                losstraj = \"losstraj\" in attack_names\n",
    "                reference = \"reference\" in attack_names\n",
    "                lira = \"lira\" in attack_names\n",
    "                calibration = \"calibration\" in attack_names\n",
    "\n",
    "                # filter attack_names\n",
    "                attack_names = [attack for attack in attack_names if attack in attack_list]\n",
    "\n",
    "                if len(attack_names) == 1:\n",
    "                    continue\n",
    "\n",
    "                # calculate cost\n",
    "                cost = sum([cost_table_time[attack] for attack in attack_names])\n",
    "                # handle the case of both reference and lira are used\n",
    "                if \"reference\" in attack_names and \"lira\" in attack_names:\n",
    "                    cost -= cost_table_time[\"lira_shadow_models\"]\n",
    "                # account for number of seeds\n",
    "                cost *= num_seed\n",
    "                \n",
    "                new_entry = {\"losstraj\": losstraj, \n",
    "                             \"reference\": reference, \"lira\": lira, \"calibration\": calibration, \n",
    "                             \"dataset\": dataset, \"AUC\": auc, \"ACC\": acc, \"cost\": cost, \"num_instance\": num_seed, \"TPR@0.001FPR\": row[\"TPR@0.001FPR\"]}\n",
    "                new_entry = pd.DataFrame([new_entry]).astype(perf_df.dtypes.to_dict())\n",
    "                perf_df = pd.concat([perf_df, new_entry], ignore_index=True)\n",
    "\n",
    "        if ensemble == \"majority_vote\":\n",
    "            perf_mv_df = perf_df\n",
    "        elif ensemble == \"union\":\n",
    "            perf_union_df = perf_df\n",
    "        elif ensemble == \"intersection\":\n",
    "            perf_intersection_df = perf_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cost of each attack versus the performance of each attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting cifar10 union\n",
      "Plotting cifar100 union\n",
      "Plotting cinic10 union\n",
      "Plotting cifar10 intersection\n",
      "Plotting cifar100 intersection\n",
      "Plotting cinic10 intersection\n",
      "Plotting cifar10 majority_vote\n",
      "Plotting cifar100 majority_vote\n",
      "Plotting cinic10 majority_vote\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.legend import Legend\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Define font properties\n",
    "bold_font = FontProperties(weight='bold', size=18)\n",
    "legend_title_font = FontProperties(weight='bold', size=18)\n",
    "legend_label_font = FontProperties(weight='bold', size=18)\n",
    "\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "if not os.path.exists(path_to_save_result):\n",
    "    os.makedirs(path_to_save_result)\n",
    "\n",
    "# Define a categorical color palette for num_instance\n",
    "colors = sns.color_palette(\"Set2\", n_colors=5)  # Generate 5 distinct colors\n",
    "color_mapping = {i: colors[i - 2] for i in range(2, 7)}  # Map num_instance values (2 to 6) to colors\n",
    "\n",
    "for ensemble in ensemble_method_list:\n",
    "    if ensemble == \"majority_vote\":\n",
    "        perf_df = perf_mv_df\n",
    "    elif ensemble == \"union\":\n",
    "        perf_df = perf_union_df\n",
    "    elif ensemble == \"intersection\":\n",
    "        perf_df = perf_intersection_df\n",
    "\n",
    "    perf_df[\"marker\"] = perf_df.apply(get_marker, axis=1)\n",
    "\n",
    "    for dataset in dataset_list:\n",
    "        print(f\"Plotting {dataset} {ensemble}\")\n",
    "        df = perf_df[perf_df[\"dataset\"] == dataset]\n",
    "\n",
    "        # Initialize the plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        # Store handles and labels for custom marker legend\n",
    "        custom_handles = []\n",
    "        custom_labels = []\n",
    "\n",
    "        # Store handles and labels for color legend\n",
    "        color_handles = []\n",
    "        color_labels = []\n",
    "\n",
    "        # Plot each marker group\n",
    "        for marker, group_df in df.groupby(\"marker\"):\n",
    "            # Get corresponding combination for the marker\n",
    "            combination = [key for key, value in ensemble_markers_mapping.items() if value == marker][0]\n",
    "            label = \", \".join(combination)  # Create a readable label\n",
    "            \n",
    "            scatter = plt.scatter(\n",
    "                group_df[\"cost\"],\n",
    "                group_df[\"TPR@0.001FPR\"],\n",
    "                marker=marker,\n",
    "                c=group_df[\"num_instance\"].map(color_mapping),  # Map num_instance to categorical colors\n",
    "                s=100,\n",
    "                alpha=0.8,\n",
    "                edgecolor='k'  # Optional: add black edges for visibility\n",
    "            )\n",
    "            \n",
    "            plt.xticks(fontsize=16, fontweight='bold')\n",
    "            plt.yticks(fontsize=16, fontweight='bold')\n",
    "\n",
    "            # Add marker handles for the legend\n",
    "            custom_handles.append(plt.Line2D([0], [0], color='black', marker=marker, linestyle='', markersize=10))\n",
    "            custom_labels.append(label)\n",
    "\n",
    "        # Create color legend\n",
    "        for num_instance, color in color_mapping.items():\n",
    "            color_handles.append(plt.Line2D([0], [0], color=color, marker='o', linestyle='', markersize=10))\n",
    "            color_labels.append(f\"{num_instance} instances\")\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel(\"Cost (minutes)\", fontproperties=bold_font)\n",
    "        plt.ylabel(\"TPR@0.1%FPR\", fontproperties=bold_font)\n",
    "\n",
    "        # Save plot without marker legend\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{path_to_save_result}/{dataset}_{ensemble}_cost_vs_perf.pdf\",\n",
    "                    bbox_inches='tight', format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "        # Save marker legend as a separate PDF\n",
    "        fig, ax = plt.subplots()\n",
    "        legend = Legend(ax, custom_handles, custom_labels, loc='center', frameon=False,\n",
    "                prop=legend_label_font)\n",
    "        ax.add_artist(legend)\n",
    "        ax.axis('off')  # Remove axes for clean legend\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{path_to_save_result}/{dataset}_{ensemble}_marker_legend.pdf\", format='pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Save color legend as a separate PDF\n",
    "        fig, ax = plt.subplots()\n",
    "        color_legend = Legend(ax, color_handles, color_labels, loc='center', frameon=False,\n",
    "                      prop=legend_label_font)\n",
    "        ax.add_artist(color_legend)\n",
    "        ax.axis('off')  # Remove axes for clean legend\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{path_to_save_result}/{dataset}_{ensemble}_color_legend.pdf\", format='pdf', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
